{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Prediction Intervals for Gradient Boosting Regression\n",
    "\n",
    "This example shows how quantile regression can be used to create prediction\n",
    "intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data for a synthetic regression problem by applying the\n",
    "function f to uniformly sampled random inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef f(x):\\n    \"\"\"The function to predict.\"\"\"\\n    return x * np.sin(x)\\n\\n\\nrng = np.random.RandomState(42)\\nX = np.atleast_2d(rng.uniform(0, 10.0, size=1000)).T\\nexpected_y = f(X).ravel()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "def f(x):\n",
    "    \"\"\"The function to predict.\"\"\"\n",
    "    return x * np.sin(x)\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = np.atleast_2d(rng.uniform(0, 10.0, size=1000)).T\n",
    "expected_y = f(X).ravel()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(\"D:\\\\Github\\\\Capstone-project\\\\Data folder\\\\training_data_to_use.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['player_type', 'games', 'games_started', 'mp_per_g', 'fg_per_g', 'fga_per_g', 'fg2_per_g', 'fg2a_per_g', 'fg3_per_g', 'fg3a_per_g', 'ft_per_g', 'fta_per_g', 'orb_per_g', 'drb_per_g', 'trb_per_g', 'ast_per_g', 'stl_per_g', 'blk_per_g', 'tov_per_g', 'pf_per_g', 'pts_per_g', 'sos', 'mp', 'fg', 'fga', 'fg2', 'fg2a', 'fg3', 'fg3a', 'ft', 'fta', 'orb', 'drb', 'trb', 'ast', 'stl', 'blk', 'tov', 'pf', 'pts', 'fg_per_min', 'fga_per_min', 'fg2_per_min', 'fg2a_per_min', 'fg3_per_min', 'fg3a_per_min', 'ft_per_min', 'fta_per_min', 'trb_per_min', 'ast_per_min', 'stl_per_min', 'blk_per_min', 'tov_per_min', 'pf_per_min', 'pts_per_min', 'fg_per_poss', 'fga_per_poss', 'fg2_per_poss', 'fg2a_per_poss', 'fg3_per_poss', 'fg3a_per_poss', 'ft_per_poss', 'fta_per_poss', 'trb_per_poss', 'ast_per_poss', 'stl_per_poss', 'blk_per_poss', 'tov_per_poss', 'pf_per_poss', 'pts_per_poss', 'off_rtg', 'def_rtg', 'per', 'ts_pct', 'efg_pct', 'fg3a_per_fga_pct', 'fta_per_fga_pct', 'pprod', 'orb_pct', 'drb_pct', 'trb_pct', 'ast_pct', 'stl_pct', 'blk_pct', 'tov_pct', 'usg_pct', 'ows', 'dws', 'ws', 'ws_per_40', 'obpm', 'dbpm', 'bpm', 'year', 'Ht', 'Wt', 'G', 'S', 'X.Min', 'ORtg', 'X.Poss', 'X.Shots', 'eFG.', 'TS.', 'OR.', 'DR.', 'ARate', 'TORate', 'Blk.', 'Stl.', 'FC.40', 'FD.40', 'FTRate', 'Pct.2', 'X.Pct', 'X.Pct.1', 'Season_x', 'Tempo_x', 'RankTempo_x', 'AdjTempo_x', 'RankAdjTempo_x', 'OE_x', 'RankOE_x', 'AdjOE_x', 'RankAdjOE_x', 'DE_x', 'RankDE_x', 'AdjDE_x', 'RankAdjDE_x', 'AdjEM_x', 'RankAdjEM_x', 'seed_x', 'Season_y', 'Tempo_y', 'RankTempo_y', 'AdjTempo_y', 'RankAdjTempo_y', 'OE_y', 'RankOE_y', 'AdjOE_y', 'RankAdjOE_y', 'DE_y', 'RankDE_y', 'AdjDE_y', 'RankAdjDE_y', 'AdjEM_y', 'RankAdjEM_y', 'seed_y', 'Year_Conf_x', 'Conf_Rk_x', 'Conf_Rating_x', 'Year_Conf_y', 'Conf_Rk_y', 'Conf_Rating_y', 'year_max_x', 'games_max_x', 'year_max_y', 'games_max_y', 'Team_AdjEM_dif', 'KP_Rk_Dif', 'KP_Conf_Dif', 'Calc A', 'Calc A2', 'Calc A3', 'Calc B', 'Calc C', 'Calc Overall A', 'Calc Overall A2', 'Calc Overall A3']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7681, 169)\n"
     ]
    }
   ],
   "source": [
    "master_table  = df[columns]\n",
    "print(master_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7681, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['offense'].to_frame()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data with 90% in training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(master_table, y, random_state=0,\n",
    "                                    train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the problem interesting, we generate observations of the target y as\n",
    "the sum of a deterministic term computed by the function f and a random noise\n",
    "term that follows a centered [log-normal](https://en.wikipedia.org/wiki/Log-normal_distribution). To make this even\n",
    "more interesting we consider the case where the amplitude of the noise\n",
    "depends on the input variable x (heteroscedastic noise).\n",
    "\n",
    "The lognormal distribution is non-symmetric and long tailed: observing large\n",
    "outliers is likely but it is impossible to observe small outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigma = 0.5 + X.ravel() / 10\n",
    "#noise = rng.lognormal(sigma=sigma) - np.exp(sigma**2 / 2)\n",
    "#y = expected_y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train, test datasets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting non-linear quantile and least squares regressors\n",
    "\n",
    "Fit gradient boosting models trained with the quantile loss and\n",
    "alpha=0.05, 0.5, 0.95.\n",
    "\n",
    "The models obtained for alpha=0.05 and alpha=0.95 produce a 90% confidence\n",
    "interval (95% - 5% = 90%).\n",
    "\n",
    "The model trained with alpha=0.5 produces a regression of the median: on\n",
    "average, there should be the same number of target observations above and\n",
    "below the predicted values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_pinball_loss, mean_squared_error\n",
    "\n",
    "all_models = {}\n",
    "common_params = dict(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=9,\n",
    "    min_samples_split=9,\n",
    ")\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that :class:`~sklearn.ensemble.HistGradientBoostingRegressor` is much\n",
    "faster than :class:`~sklearn.ensemble.GradientBoostingRegressor` starting with\n",
    "intermediate datasets (`n_samples >= 10_000`), which is not the case of the\n",
    "present example.\n",
    "\n",
    "For the sake of comparison, we also fit a baseline model trained with the\n",
    "usual (mean) squared error (MSE).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "gbr_ls = GradientBoostingRegressor(loss=\"squared_error\", **common_params)\n",
    "all_models[\"mse\"] = gbr_ls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an evenly spaced evaluation set of input values spanning the [0, 10]\n",
    "range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xx = np.atleast_2d(np.linspace(0, 10, 1000)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the true conditional mean function f, the predictions of the conditional\n",
    "mean (loss equals squared error), the conditional median and the conditional\n",
    "90% interval (from 5th to 95th conditional percentiles).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1 features, but GradientBoostingRegressor is expecting 169 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mall_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m y_lower \u001b[38;5;241m=\u001b[39m all_models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq 0.05\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(xx)\n\u001b[0;32m      5\u001b[0m y_upper \u001b[38;5;241m=\u001b[39m all_models[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq 0.95\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(xx)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1798\u001b[0m, in \u001b[0;36mGradientBoostingRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict regression target for X.\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m \n\u001b[0;32m   1786\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1798\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m     \u001b[38;5;66;03m# In regression we can directly return the raw value from the trees.\u001b[39;00m\n\u001b[0;32m   1802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1 features, but GradientBoostingRegressor is expecting 169 features as input."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = all_models[\"mse\"].predict(xx)\n",
    "y_lower = all_models[\"q 0.05\"].predict(xx)\n",
    "y_upper = all_models[\"q 0.95\"].predict(xx)\n",
    "y_med = all_models[\"q 0.50\"].predict(xx)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n",
    "plt.plot(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\n",
    "plt.plot(xx, y_med, \"r-\", label=\"Predicted median\")\n",
    "plt.plot(xx, y_pred, \"r-\", label=\"Predicted mean\")\n",
    "plt.plot(xx, y_upper, \"k-\")\n",
    "plt.plot(xx, y_lower, \"k-\")\n",
    "plt.fill_between(\n",
    "    xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\n",
    ")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.ylim(-10, 25)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the predicted median with the predicted mean, we note that the\n",
    "median is on average below the mean as the noise is skewed towards high\n",
    "values (large outliers). The median estimate also seems to be smoother\n",
    "because of its natural robustness to outliers.\n",
    "\n",
    "Also observe that the inductive bias of gradient boosting trees is\n",
    "unfortunately preventing our 0.05 quantile to fully capture the sinoisoidal\n",
    "shape of the signal, in particular around x=8. Tuning hyper-parameters can\n",
    "reduce this effect as shown in the last part of this notebook.\n",
    "\n",
    "## Analysis of the error metrics\n",
    "\n",
    "Measure the models with :func:`~sklearn.metrics.mean_squared_error` and\n",
    ":func:`~sklearn.metrics.mean_pinball_loss` metrics on the training dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a3f70_row0_col3, #T_a3f70_row1_col0, #T_a3f70_row2_col1, #T_a3f70_row3_col2 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a3f70\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a3f70_level0_col0\" class=\"col_heading level0 col0\" >pbl=0.05</th>\n",
       "      <th id=\"T_a3f70_level0_col1\" class=\"col_heading level0 col1\" >pbl=0.50</th>\n",
       "      <th id=\"T_a3f70_level0_col2\" class=\"col_heading level0 col2\" >pbl=0.95</th>\n",
       "      <th id=\"T_a3f70_level0_col3\" class=\"col_heading level0 col3\" >MSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a3f70_level0_row0\" class=\"row_heading level0 row0\" >mse</th>\n",
       "      <td id=\"T_a3f70_row0_col0\" class=\"data row0 col0\" >54.634142</td>\n",
       "      <td id=\"T_a3f70_row0_col1\" class=\"data row0 col1\" >54.634142</td>\n",
       "      <td id=\"T_a3f70_row0_col2\" class=\"data row0 col2\" >54.634142</td>\n",
       "      <td id=\"T_a3f70_row0_col3\" class=\"data row0 col3\" >20306.630550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3f70_level0_row1\" class=\"row_heading level0 row1\" >q 0.05</th>\n",
       "      <td id=\"T_a3f70_row1_col0\" class=\"data row1 col0\" >13.200287</td>\n",
       "      <td id=\"T_a3f70_row1_col1\" class=\"data row1 col1\" >112.238654</td>\n",
       "      <td id=\"T_a3f70_row1_col2\" class=\"data row1 col2\" >211.277021</td>\n",
       "      <td id=\"T_a3f70_row1_col3\" class=\"data row1 col3\" >73942.009424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3f70_level0_row2\" class=\"row_heading level0 row2\" >q 0.50</th>\n",
       "      <td id=\"T_a3f70_row2_col0\" class=\"data row2 col0\" >50.857642</td>\n",
       "      <td id=\"T_a3f70_row2_col1\" class=\"data row2 col1\" >54.538733</td>\n",
       "      <td id=\"T_a3f70_row2_col2\" class=\"data row2 col2\" >58.219823</td>\n",
       "      <td id=\"T_a3f70_row2_col3\" class=\"data row2 col3\" >21326.145498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3f70_level0_row3\" class=\"row_heading level0 row3\" >q 0.95</th>\n",
       "      <td id=\"T_a3f70_row3_col0\" class=\"data row3 col0\" >232.013941</td>\n",
       "      <td id=\"T_a3f70_row3_col1\" class=\"data row3 col1\" >123.561350</td>\n",
       "      <td id=\"T_a3f70_row3_col2\" class=\"data row3 col2\" >15.108760</td>\n",
       "      <td id=\"T_a3f70_row3_col3\" class=\"data row3 col3\" >82809.985903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22b9feaccd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def highlight_min(x):\n",
    "    x_min = x.min()\n",
    "    return [\"font-weight: bold\" if v == x_min else \"\" for v in x]\n",
    "\n",
    "\n",
    "results = []\n",
    "for name, gbr in sorted(all_models.items()):\n",
    "    metrics = {\"model\": name}\n",
    "    y_pred = gbr.predict(X_train)\n",
    "    for alpha in [0.05, 0.5, 0.95]:\n",
    "        metrics[\"pbl=%1.2f\" % alpha] = mean_pinball_loss(y_train, y_pred, alpha=alpha)\n",
    "    metrics[\"MSE\"] = mean_squared_error(y_train, y_pred)\n",
    "    results.append(metrics)\n",
    "\n",
    "pd.DataFrame(results).set_index(\"model\").style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One column shows all models evaluated by the same metric. The minimum number\n",
    "on a column should be obtained when the model is trained and measured with\n",
    "the same metric. This should be always the case on the training set if the\n",
    "training converged.\n",
    "\n",
    "Note that because the target distribution is asymmetric, the expected\n",
    "conditional mean and conditional median are significantly different and\n",
    "therefore one could not use the squared error model get a good estimation of\n",
    "the conditional median nor the converse.\n",
    "\n",
    "If the target distribution were symmetric and had no outliers (e.g. with a\n",
    "Gaussian noise), then median estimator and the least squares estimator would\n",
    "have yielded similar predictions.\n",
    "\n",
    "We then do the same on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7dc90_row0_col1, #T_7dc90_row0_col3, #T_7dc90_row1_col0, #T_7dc90_row3_col2 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7dc90\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7dc90_level0_col0\" class=\"col_heading level0 col0\" >pbl=0.05</th>\n",
       "      <th id=\"T_7dc90_level0_col1\" class=\"col_heading level0 col1\" >pbl=0.50</th>\n",
       "      <th id=\"T_7dc90_level0_col2\" class=\"col_heading level0 col2\" >pbl=0.95</th>\n",
       "      <th id=\"T_7dc90_level0_col3\" class=\"col_heading level0 col3\" >MSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc90_level0_row0\" class=\"row_heading level0 row0\" >mse</th>\n",
       "      <td id=\"T_7dc90_row0_col0\" class=\"data row0 col0\" >56.598382</td>\n",
       "      <td id=\"T_7dc90_row0_col1\" class=\"data row0 col1\" >56.566344</td>\n",
       "      <td id=\"T_7dc90_row0_col2\" class=\"data row0 col2\" >56.534306</td>\n",
       "      <td id=\"T_7dc90_row0_col3\" class=\"data row0 col3\" >22297.558739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc90_level0_row1\" class=\"row_heading level0 row1\" >q 0.05</th>\n",
       "      <td id=\"T_7dc90_row1_col0\" class=\"data row1 col0\" >13.888905</td>\n",
       "      <td id=\"T_7dc90_row1_col1\" class=\"data row1 col1\" >112.780787</td>\n",
       "      <td id=\"T_7dc90_row1_col2\" class=\"data row1 col2\" >211.672668</td>\n",
       "      <td id=\"T_7dc90_row1_col3\" class=\"data row1 col3\" >74356.494268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc90_level0_row2\" class=\"row_heading level0 row2\" >q 0.50</th>\n",
       "      <td id=\"T_7dc90_row2_col0\" class=\"data row2 col0\" >53.722131</td>\n",
       "      <td id=\"T_7dc90_row2_col1\" class=\"data row2 col1\" >56.798942</td>\n",
       "      <td id=\"T_7dc90_row2_col2\" class=\"data row2 col2\" >59.875752</td>\n",
       "      <td id=\"T_7dc90_row2_col3\" class=\"data row2 col3\" >22771.498831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dc90_level0_row3\" class=\"row_heading level0 row3\" >q 0.95</th>\n",
       "      <td id=\"T_7dc90_row3_col0\" class=\"data row3 col0\" >235.121506</td>\n",
       "      <td id=\"T_7dc90_row3_col1\" class=\"data row3 col1\" >126.225563</td>\n",
       "      <td id=\"T_7dc90_row3_col2\" class=\"data row3 col2\" >17.329621</td>\n",
       "      <td id=\"T_7dc90_row3_col3\" class=\"data row3 col3\" >85483.931782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22ba3e64ee0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for name, gbr in sorted(all_models.items()):\n",
    "    metrics = {\"model\": name}\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    for alpha in [0.05, 0.5, 0.95]:\n",
    "        metrics[\"pbl=%1.2f\" % alpha] = mean_pinball_loss(y_test, y_pred, alpha=alpha)\n",
    "    metrics[\"MSE\"] = mean_squared_error(y_test, y_pred)\n",
    "    results.append(metrics)\n",
    "\n",
    "pd.DataFrame(results).set_index(\"model\").style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors are higher meaning the models slightly overfitted the data. It still\n",
    "shows that the best test metric is obtained when the model is trained by\n",
    "minimizing this same metric.\n",
    "\n",
    "Note that the conditional median estimator is competitive with the squared\n",
    "error estimator in terms of MSE on the test set: this can be explained by\n",
    "the fact the squared error estimator is very sensitive to large outliers\n",
    "which can cause significant overfitting. This can be seen on the right hand\n",
    "side of the previous plot. The conditional median estimator is biased\n",
    "(underestimation for this asymmetric noise) but is also naturally robust to\n",
    "outliers and overfits less.\n",
    "\n",
    "\n",
    "## Calibration of the confidence interval\n",
    "\n",
    "We can also evaluate the ability of the two extreme quantile estimators at\n",
    "producing a well-calibrated conditional 90%-confidence interval.\n",
    "\n",
    "To do this we can compute the fraction of observations that fall between the\n",
    "predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to Series, length must be 1: given 6912",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoverage_fraction\u001b[39m(y, y_low, y_high):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mlogical_and(y \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m y_low, y \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y_high))\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcoverage_fraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mq 0.05\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mq 0.95\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m, in \u001b[0;36mcoverage_fraction\u001b[1;34m(y, y_low, y_high)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoverage_fraction\u001b[39m(y, y_low, y_high):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mlogical_and(\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_low\u001b[49m, y \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y_high))\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\arraylike.py:62\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:7579\u001b[0m, in \u001b[0;36mDataFrame._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   7576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cmp_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   7577\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# only relevant for Series other case\u001b[39;00m\n\u001b[1;32m-> 7579\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_method_FRAME\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   7581\u001b[0m     \u001b[38;5;66;03m# See GH#4537 for discussion of scalar op behavior\u001b[39;00m\n\u001b[0;32m   7582\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_frame_op(other, op, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:248\u001b[0m, in \u001b[0;36malign_method_FRAME\u001b[1;34m(left, right, axis, flex, level)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m right\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 248\u001b[0m         right \u001b[38;5;241m=\u001b[39m \u001b[43mto_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m right\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m right\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m left\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:239\u001b[0m, in \u001b[0;36malign_method_FRAME.<locals>.to_series\u001b[1;34m(right)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(right):\n\u001b[1;32m--> 239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m             msg\u001b[38;5;241m.\u001b[39mformat(req_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcolumns), given_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(right))\n\u001b[0;32m    241\u001b[0m         )\n\u001b[0;32m    242\u001b[0m     right \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_constructor_sliced(right, index\u001b[38;5;241m=\u001b[39mleft\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m right\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to coerce to Series, length must be 1: given 6912"
     ]
    }
   ],
   "source": [
    "def coverage_fraction(y, y_low, y_high):\n",
    "    return np.mean(np.logical_and(y >= y_low, y <= y_high))\n",
    "\n",
    "\n",
    "coverage_fraction(\n",
    "    y_train,\n",
    "    all_models[\"q 0.05\"].predict(X_train),\n",
    "    all_models[\"q 0.95\"].predict(X_train),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the training set the calibration is very close to the expected coverage\n",
    "value for a 90% confidence interval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to Series, length must be 1: given 769",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcoverage_fraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mq 0.05\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mq 0.95\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m, in \u001b[0;36mcoverage_fraction\u001b[1;34m(y, y_low, y_high)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoverage_fraction\u001b[39m(y, y_low, y_high):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mlogical_and(\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_low\u001b[49m, y \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y_high))\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\arraylike.py:62\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:7579\u001b[0m, in \u001b[0;36mDataFrame._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   7576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cmp_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   7577\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# only relevant for Series other case\u001b[39;00m\n\u001b[1;32m-> 7579\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_method_FRAME\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   7581\u001b[0m     \u001b[38;5;66;03m# See GH#4537 for discussion of scalar op behavior\u001b[39;00m\n\u001b[0;32m   7582\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_frame_op(other, op, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:248\u001b[0m, in \u001b[0;36malign_method_FRAME\u001b[1;34m(left, right, axis, flex, level)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m right\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 248\u001b[0m         right \u001b[38;5;241m=\u001b[39m \u001b[43mto_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m right\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m right\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m left\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:239\u001b[0m, in \u001b[0;36malign_method_FRAME.<locals>.to_series\u001b[1;34m(right)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(right):\n\u001b[1;32m--> 239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m             msg\u001b[38;5;241m.\u001b[39mformat(req_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcolumns), given_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(right))\n\u001b[0;32m    241\u001b[0m         )\n\u001b[0;32m    242\u001b[0m     right \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_constructor_sliced(right, index\u001b[38;5;241m=\u001b[39mleft\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m right\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to coerce to Series, length must be 1: given 769"
     ]
    }
   ],
   "source": [
    "coverage_fraction(\n",
    "    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set, the estimated confidence interval is slightly too narrow.\n",
    "Note, however, that we would need to wrap those metrics in a cross-validation\n",
    "loop to assess their variability under data resampling.\n",
    "\n",
    "## Tuning the hyper-parameters of the quantile regressors\n",
    "\n",
    "In the plot above, we observed that the 5th percentile regressor seems to\n",
    "underfit and could not adapt to sinusoidal shape of the signal.\n",
    "\n",
    "The hyper-parameters of the model were approximately hand-tuned for the\n",
    "median regressor and there is no reason that the same hyper-parameters are\n",
    "suitable for the 5th percentile regressor.\n",
    "\n",
    "To confirm this hypothesis, we tune the hyper-parameters of a new regressor\n",
    "of the 5th percentile by selecting the best model parameters by\n",
    "cross-validation on the pinball loss with alpha=0.05:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from pprint import pprint\n",
    "\n",
    "param_grid = dict(\n",
    "    learning_rate=[0.05, 0.1, 0.2],\n",
    "    max_depth=[2, 5, 10],\n",
    "    min_samples_leaf=[1, 5, 10, 20],\n",
    "    min_samples_split=[5, 10, 20, 30, 50],\n",
    ")\n",
    "alpha = 0.05\n",
    "neg_mean_pinball_loss_05p_scorer = make_scorer(\n",
    "    mean_pinball_loss,\n",
    "    alpha=alpha,\n",
    "    greater_is_better=False,  # maximize the negative loss\n",
    ")\n",
    "gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, random_state=0)\n",
    "search_05p = HalvingRandomSearchCV(\n",
    "    gbr,\n",
    "    param_grid,\n",
    "    resource=\"n_estimators\",\n",
    "    max_resources=250,\n",
    "    min_resources=50,\n",
    "    scoring=neg_mean_pinball_loss_05p_scorer,\n",
    "    n_jobs=2,\n",
    "    random_state=0,\n",
    ").fit(X_train, y_train)\n",
    "pprint(search_05p.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the hyper-parameters that were hand-tuned for the median\n",
    "regressor are in the same range as the hyper-parameters suitable for the 5th\n",
    "percentile regressor.\n",
    "\n",
    "Let's now tune the hyper-parameters for the 95th percentile regressor. We\n",
    "need to redefine the `scoring` metric used to select the best model, along\n",
    "with adjusting the alpha parameter of the inner gradient boosting estimator\n",
    "itself:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "alpha = 0.95\n",
    "neg_mean_pinball_loss_95p_scorer = make_scorer(\n",
    "    mean_pinball_loss,\n",
    "    alpha=alpha,\n",
    "    greater_is_better=False,  # maximize the negative loss\n",
    ")\n",
    "search_95p = clone(search_05p).set_params(\n",
    "    estimator__alpha=alpha,\n",
    "    scoring=neg_mean_pinball_loss_95p_scorer,\n",
    ")\n",
    "search_95p.fit(X_train, y_train)\n",
    "pprint(search_95p.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that the hyper-parameters for the 95th percentile regressor\n",
    "identified by the search procedure are roughly in the same range as the hand-\n",
    "tuned hyper-parameters for the median regressor and the hyper-parameters\n",
    "identified by the search procedure for the 5th percentile regressor. However,\n",
    "the hyper-parameter searches did lead to an improved 90% confidence interval\n",
    "that is comprised by the predictions of those two tuned quantile regressors.\n",
    "Note that the prediction of the upper 95th percentile has a much coarser shape\n",
    "than the prediction of the lower 5th percentile because of the outliers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lower = search_05p.predict(xx)\n",
    "y_upper = search_95p.predict(xx)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n",
    "plt.plot(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\n",
    "plt.plot(xx, y_upper, \"k-\")\n",
    "plt.plot(xx, y_lower, \"k-\")\n",
    "plt.fill_between(\n",
    "    xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\n",
    ")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.ylim(-10, 25)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Prediction with tuned hyper-parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot looks qualitatively better than for the untuned models, especially\n",
    "for the shape of the of lower quantile.\n",
    "\n",
    "We now quantitatively evaluate the joint-calibration of the pair of\n",
    "estimators:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_fraction(y_train, search_05p.predict(X_train), search_95p.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_fraction(y_test, search_05p.predict(X_test), search_95p.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibration of the tuned pair is sadly not better on the test set: the\n",
    "width of the estimated confidence interval is still too narrow.\n",
    "\n",
    "Again, we would need to wrap this study in a cross-validation loop to\n",
    "better assess the variability of those estimates.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
